Okay, enough dilly-dallying. I think I've run through enough examples that I
know what I want the language to do.

#+TITLE: An Attempt to Implement

* Use
  The file =src/repl.jl= contains the initial interpreter entry point.

  Running =repl(initenv)= will get you a *very* bare repl that will read
  commands until EOF (ctrl-d).

* Current problems
  I've gotten off track, though the track was never very well defined.

  - [X] Read metadata (^)
  - [ ] Reader quote (')
  - [ ] Move to fexprs for the language core
    With fexprs, all of the "special forms" of lisp can be defined as env
    macros.

    I need a new name for these metaprocessors. "interpreter" is the word I've
    been using, but it's too overloaded. The hierarchy is basically 1) macros
    that can access the env and current continuations 2) "syntax" macros that
    can postpone evaluation of their arguments by receiving unevaluated
    arguments and returning an sexp which will be evaluated (but can't call
    `eval` directly since they haven't an environment to pass to it 3) functions
    (well this things I'm calling functions anyway. They're not mathematical
    functions, more like ordered relations.

    What I called syntax macros and functions above are really just restrictions
    of full macros. Very convenient restrictions because the encapsulate
    patterns that make code much easier to understand and work with.
  - [ ] Scheduler based message passing
    To be able to 1) return control to multiple places and 2) work steal, we
    need a way to wrap work into tasks and queue them up.

    The best way I can think of to do that at the moment is to have =emit= and
    =recur= pass control to the scheduler along with a list of tasks. All the
    scheduler will do initially is enqueue all but the first task and
    immediately step into the first one.

    The base continuation should also pass control to the scheduler, but with no
    new tasks. That way any task which ends in =(emit)= or equivalent will
    immediately lead to the next task being read off the queue and entered.

    Similarly reading from streams that don't yet have values should park and
    return control to the scheduler to go somewhere else.

    We need a basic fairness criterion (like time waited) to prevent
    starvation. But some tasks are more important, specifically tasks which only
    provide values being waited upon. Maybe those events shouldn't be tasks, but
    should execute immediately (and step into the waiting task?).

    I still need to think about how to coordinate waiting vs ready tasks.
  - [ ] Polymorphic dispatch (in xprl)
  - [ ] Decide abount continuations in env
    The continuation is part of the environment of a function. It's (normally) a
    lexical construct in that control goes back to the caller. It can be
    diverted for more exotic control flows, but code needs both an environment
    and a continuation to be sensibly evaluated.

    That said, I keep mutating the continuation too much. It seems like
    excessive env manipulation, but maybe I just need a higher level interface
    too it like `extend` for bindings.
  - [ ] Bootstrap intensionality
    You can define `def` as a macro. You can even define `macro` as a macro. The
    problem is that this leads to cyclic references in the environment. If we
    look at invocation as giving new names to existing quantites and definition
    as situating a fragment of code (a syntactic contstruct, one of Witt's
    pictures) into a context to give it meaning, then it quickly becomes
    apparent that there must be a non-empty initial environment in order for any
    expression to have meaning.

    That initial environment must contain quantities defined by extension
    (from the outside).

    evaluation of a definition produces a structure (a value), but structures
    themselves are defined by code. So we can't write the initial definition of
    `macro` in "xprl in julia", unless we leave the entire semantics of data
    extensional. And that's not acceptable for a language based on passing
    values.

    Except it might not be possible. I'll need to experiment with how far down I
    can push intensionality, but at some point data must have a binary layout
    whose shape and meaning are in my head, not the code.

    Can every extensional definition be expressed as an external interpreter to
    which we send messages?

    I think that's the right way to look at it. Primitives are units of code
    written in another language. There must be primitives, otherwise we'd have
    Hilbert's program acheived.

    So if every primitive is just code in another language which (presumably)
    takes parameters passed from our language, is that not just an interpreter?

    It doesn't need to be a full fledged =lli= type of interpreter. Just an
    =imul= instruction is an interpreter that does one thing.

    The goal of this task is to make every non intentional definition an
    explicit message passing call to another interpreter (i.e. not =eval=). What
    that will look like I still don't know.

    One problem that comes to mind is that general interpreters need to be
    sandboxed. Values passed to them must be allocated on read only pages,
    syscalls should be spoofed, everything.

    To start with, we'll need a trusted class of external interpreters and an
    "unsafe" key to allow new interpreters into this trusted zone. Then we'll
    need to hand write and verify a subset from which we can build the
    sandboxing for general interpreters.
* The Interpreters
  If we're going to work with interpreters all the way down, then we'll need to
  define a minimal set with which to have a useful language.
** eval
   This is pretty straight forward.
** Constructors
   ^Macro, ^Fn, tagged types.
** Value Manipulation
   All of the protocols of lists, vectors, maps, and sets are primitive
   interpreters, just as =cons= was a necessary special form in lisp. Note that
   contructors for values are not primitive, since list, map, et al. are just
   implemented by (reduce conj ...).
** Arithmetic
   You can't compute much without arithmetic. We only need 3 arithmetic
   primitives, call them =compute=, =simplify=, and =approximate=.

   =compute= takes an sexp like (+ 7 9) and returns a structure of the form
   ^Sum [7 9].

   =approximate= takes a compute structure and an integer N, and returns a
   rational number within 10^{-N} of the value of the compute structure.

   =simplify= uses arithmetic identities and search heuristices to simplify
   expressions. E.g. ^Sum [7 9] => 16, (derivative (fn [x] (* x x)) 'x) => (fn [x]
   (* 2 x)), etc.  We can extend this with trignometry, calculus, various
   algrebras, etc.

   Think of =simplify= as symbolic math, and =approximate= as numerics.
** String Manipulation
   This should be reducible to list manipulation, but at the very least we'll
   need primitives for encoding/decoding ascii, utf, etc..

   Honestly, I still think that strings should be a loose tree structure. That
   way going up from sentences, to paragraphs, is symmetrical to going down to
   characters and partial code points. Something like RRB tries or Fibonnaci
   heaps are what I'm going for.

   We can probably get better memory packing by intelligently encoding subtrees
   and thus make up for the "not a single linear slice of addresses" issue.
* The Runtime
  In theory =emit= sends a message directly to a channel (and thence to any
  listeners on that channel).

  In reality, we need to map interpretation of messages to hardware in a dynamic
  fashion, so emission is mediated by queues and a scheduler.

  What is the scheduler? Is it useful to think of it as an interpreter? It is a
  meta interpreter of sorts which sees the sending of messages as messages in
  their own right. So it's a step up the reflective tower; a higher order
  process, like a message passing macro.


* The Language
** No Side Effects
   This isn't true of the program as a whole, obviously, but it is true of
   virtually all code that isn't part of the runtime itself.

   No assignment, no code blocks (`progn`, `do`, etc.), no syscalls or other
   standard forms of IO.

   Streams of messages are the basic structure. A stream is a value, effectively
   a persistent queue, but we may or may not know what the values in it are
   yet. Attempts to read past the threshold of present knowledge will lead to
   CSP style parking (coroutines).

   A function takes arguments — possibly including streams — and returns what
   I'm calling a cable. A cable is a bundle of named streams with a special,
   unnamed stream which corresponds the the "normal return" of a function.

   The building block of all synchronisation is that messages emitted on a
   function's output streams come into existence at a point in time which is
   strictly later than the messages consumed by that function (either messages
   passed in as arguments, or values read off of streams that were passed
   in). That's it: consistency exists at a single point in time and space and
   nowhere else. Everything bigger requires some method of consensus.

   A function can emit zero or more messages on any number of streams, but only
   from tail position. Thus functions effectively "return" a map from streams to
   lists of messages, and the runtime hooks those messages up to the appropriate
   queues based on the message passing topology which is our version of the call
   graph.

   Additionally a function may call itself recursively and emit messages at the
   same time. The semantics of such a (recur (emit ... expression are that
   messages are emitted between iterations of a function. Strictly after one
   call (tail position only remember) and strictly before the next invocation.

   I think of these "recur and emit" tail calls as a shorthand for emitting to
   streams that loop back into the emitting function. Such cycles from a node to
   itself have proven difficult to model except via recursion. And they're the
   only way to model changing state over time without any reference types.

   To deal with IO, there must be streams that connect to the outside
   world. Such streams include stdin, stdout, etc. as well as a function
   http/request which takes an argument and returns a cable whose default stream
   might emit a response, or whose error stream might emit an error, etc..

   There is no =put!= in the core language, so how does one write to stdout? The
   answer is =wire= (name provisional) which connects one stream to another such
   that in (wire a b) all messages emitted to `a` will be emitted to `b`. `b`
   might contain other messages as well and the order of interleaving is the
   order of receipt. If you want more coordination, you must write a stream
   processor which takes in multiple streams and decides the order of
   emission. The end programmer has the power to do this.

   But what about IO that I'm not aware of? New hardware drivers, for instance?

   The language must have an escape valve on the pattern of rust's =unsafe=
   which allows new effects to be created but isolates them from the rest of the
   program and calls attention to their dangerous character.

   But that's a work in progress within a work in progress.
** Stream processing
   Iteration is not the correct primitive to talk about stream processing.

   Iteration assumes that the stream will be consumed in uniform chunks, which
   is an assumption about arrays, stride, and padding. Properties of the
   collection being iterated over, instead of the data being processed.

   My canonical example is a tokeniser or lisp reader. The number of characters
   consumed per "step" is a function of the data on the stream and the semantics
   of the language being processed. The fact that it's a stream of utf-8
   characters is entirely orthogonal.

   A more complex example is a repl where the meanings (and thus processing) of
   chunks read off the stream depends on an environment which is built from
   reading and evaluating the stream up until now.

   The (recur (emit ...) ...) construct from the xprl examples provides a lower
   level primitive from which we can build both structural processing (standard
   map/filter/fold style iteration) as well as more complex forms of stream
   processing that derive and modify their context from the contents of the
   messages being read, rather than their encoding.

   Of course, this is somewhat moot if you can manipulate a local state to keep
   track of context, but I'm trying to avoid side effects.

** Dynamic Linking to Statically Determined Referrents
   I've gone over this in the abstract in the repo's top level readme, this is a
   snapshot of how I'm approaching it in this implementation.

   The principle taken here is that the code that gets invoked at runtime must
   be the code the programmer planned to invoke at development time. Thus
   whether a function is shipped in a binary or found in a system library is
   irrelevant. Just so long as it's the correct function.

   In the following functions carry their lexical environments around with
   them. A function is not an opaque compiled proceedure, object, etc. but a
   datastructure containing a body, arguments, and an environment in which that
   body is to be interpreted once the arguments are known.

   In code:

   #+BEGIN_SRC
   (defn apply [env ^Fn f args]
     (eval (extend (:env f) (:slots f) (map (eval env) args)) (:body f)))
   #+END_SRC

   That is, a function is just a snippet of code and an environment (with "holes"
   in it) in which to run that snippet.

   Application of a function to arguments fills those "holes" and evaluates the
   body. That's it.

   The implications are rather more complex. Firstly, when a form defining a
   function is evaluated, the environment in which is it evaluated is captured
   and stored as part of the "function" datastructure as follows:

   #+BEGIN_SRC
   (eval env '(fn args body))

   => #Fn{:env env :slots args :body body}
   #+END_SRC

   This makes cyclic reference in the environment impossible, which means we
   need combinators or trampolines for recursion. That's certainly annoying, but
   a solved problem.

   This is very static, and semantically it is, which is the point. The dynamism
   is a topic for another day.

** (Human) Language Agnostic syntax
   This is a long term goal that I've let slip recently.

   To the computer, everything is a machine word at the end of the day, so why
   does it matter whether a symbol is in English, Japanese, or Arabic? Whether
   the reader runs left to right, top to bottom, or right to left? It doesn't,
   it's just the bias built into (the American dominated) computer culture.

   Internally, a form like (fn [x y z] (conj x (g y z))) can be read in as
   ($0 [$1 $2 $3] ($4 $1 ($5 $2 $3))), with a translation table mapping those
   normalised names to both code forms and symbols. If all symbols and keywords
   are stored this way and translated on read/print, then we can swap one
   translation table for another and "translate" the program.

   The translations won't be good, but what's far more important is that we can
   *transliterate* the program, which makes it accessible to people who can't
   read the Roman alphabet. Of course they'll need translated documentation to
   understand what's going on.

   This scheme can work reasonably well, but will be an immense amount of
   work. The names of symbols and keywords are semantic: they have meaning to
   the programmer that cannot be defined fully by their context of use
   (syntax). So the translation can only be meaningfully performed by someone
   who understands the code. Maybe machine translation can do well enough to
   reduce the workload to something manageable.
* Limitations of the current implementation
  These are indexed by time they occur to me

** [2023-01-04 Wed 14:57]
   Functions can return multiple values, but only the first one is "normally"
   received.

   Wrapping a function call in the `stream` macro collects all of those values
   into a stream.

   But that has to be done statically. You can't call a function and then later
   call `stream` on it to get the "rest" of the values.

   Multiple emissions should block until someone wants to read them. That will
   save computational work by applying backpressure to unread results, but more
   importantly it will let us treat multiple returns as a "value".
