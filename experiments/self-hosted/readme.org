This is a scratchpad of what might feel like to program in xprl.

It's rapidly becoming yeat another attempt to organise a sprawling spiral of
research. That's okay; I really don't know where I want to take this yet.

Initially I'm writing the language in itself to make sure it's sufficiently
thought out to do something useful.

Bootstrapping is going to be fun.

* Reading List
   - [ ] A Simple Reflective Interpreter, Jefferson & Friedman 1996
   - [ ] Intensions and Extensions in a Reflective Tower, Danvy & Malmkjær 1988
   - [ ] The Mystery of the Tower Revealed, Wand & Friedman 1988
   - [ ] The Theory of Fexprs is Trivial, Wand 1998
     I've read the intro of this paper before and took it as a dismissal fexprs
     and mathematically uninteresting, which I believe is the general takaway in
     the community.

     But doesn't that mean that we have a solution to function equality? That is
     to say, does this paper actually prove that a lisp interpreter can be
     qualitatively more powerful than the lambda calculus? I definitely need to
     read this now. The implications for robust engineering are impossible to
     overstate if that's the case, even if the math is boring.
   - [ ] Mathematical Foundations of Joy
   - [ ] Reification: Reflection without Metaphysics; Friedman, Wand 1984
   - [ ] Brian Smith's Phd Thesis
     This thing is 762 pages long. That's just crazy. But judging by the table
     of contents, the first 150 pages or so are likely to be full of useful
     insight.
   - [ ] Reread The Early History of Smalltalk
   - [ ] Art of the Metaobject Protocol
     You know a book is above your paygrade when you read it and think "Hey they
     didn't do anything." and then realise that they've implemented an entire
     language without you noticing.
* Languages to learn more about
  - Factor
  - Joy
  - See where Unison has gotten
  - Minikanren and relational programming in general
  - CLOS
  - Go
    Comparatively boring choice? Well people who work in go day to day always
    tell me the same thing: go is boring and yet incredibly productive. They get
    everything done and go home early on a regular basis.

    That is the most understated attestation of excellence I've ever heard. I
    want to see it in action. Cool is only useful to draw the crowd.
* Questions
** [2022-09-28 Wed 12:26] def, intern, and purity
   If there are no side effects, how does one implement `def`?

   You don't. We need side effects somewhere, but they have to be constrained to
   the communication layer.

   I think of the communication layer as a hypergraph (though I keep coming back
   to the idea of using symplectic topology to analyse it, so maybe simplicial
   complexes are a better foundation...) where the edges are the emission
   channels (one writer, potentially many readers) and the nodes are either

   1) Pure computations which commence when a message is available on each input
      channel and terminate with a map from channels to lists of messages.
   2) Sources, which take no input, but emit (potentially infinitely many)
      messages to their output channels.
   3) Sinks, which receive messages but emit nothing.

   Sources and sinks are the edge conditions of the system. Sources allow
   repeatable interaction with things like time, PRNGs, etc. by logging the
   messages.

   Sinks, on the otherhand are the escape valve that lets us do anything we have
   to do. Sinks have to able to do anything, otherwise we can't implement the
   language, but they also need to be heavily restricted most of the time,
   otherwise we'll never be able to understand what a program might do.

   To implement `intern`, we would need a sink/source pair where the sink
   receives messages saying "merge this form into the trie", and the source
   emits messages saying "Ref has been merged into tree". The actual magic lives
   in the gap between sink and source.

   Sending messages over a network is the same sort of proposition. We need a
   sink that takes request data, creates sources which will eventually emit
   reponse data (or errors), sends those new sources somewhere, then sends the
   request and sets up the response listeners.

   It seems painfully intricate and potentially a point of failure. But I hope
   that pushing these details to the edge of the system will make the centre
   much easier to manipulate and reason about. Time will tell.

** [2022-09-28 Wed 12:42] Multimethods and static linking
   The biggest failing point of multimethods, in my experience, is that they are
   global mutable variables, so suddenly the behaviour of your program depends
   upon the order in which code modules get loaded.

   Ultimately it's unavoidable that the compiler has to know about the code you
   want to call before it can emit the code for the call.

   My solution (at present) is to make it so that polymorphism is restricted to
   the set of methods known to the reader when the code making the recursive
   call is read. That way the developer can inspect the set of possible methods
   (fixed), and make sure the one they expect is present. The actual dispatch
   still happens at runtime, but the choices are fixed at dev time. Incidentally
   it should also be possible for the developer to add annotations reducing the
   size of the set of possible implementations to 1, thus ensuring the jit will
   insert a direct call, when that's needed.

   The two layers of buzzpop should make this simple to implement. Every
   concrete method is interned in the form trie, but when a name is overridden,
   one of two things must happen.

   1) If the name is known to be a simple indirection, then the name trie gets
      updated, and you need to use time travel to find what the name used to be
      for things read in in the past.
   2) If the old and new versions of the name point to indirect indirections,
      then we can merge those indirect indirections. Note that the trie is still
      updated with history so that previous versions of the dispatch table can
      be referred to. This allows one symbol to point to different sets of
      methods depending on the relative points at which the references and
      definitions of that symbol are read.

   That sounds absurdly complicated. And it is. But that complication is
   inherent in the problem of building an intertwingled dynamic system by
   linearly scanning source files.

   One of my core goals is to prevent the programmer from being able to lie to
   themselves about what they do and do not know.

** [2022-10-06 Thu 09:19] Context and fexprs
   The most common issue I've been having with a complete lack of side effects
   is the maintenance of local state. The language itself needs to keep internal
   state so that new defs can be referred to later on.

   Modelling state as function sending results back to two locations is a
   kludge. It's not that dissimilar to the state monad in that it keeps state
   hidden away inside some secret loop that isn't readily accessible except when
   necessary.

   That's the wrong way to go about it entirely.

** [2022-10-06 Thu 10:23] Reflection and Semantics in Lisp
   Brian Cantwell Smith 1984

   I'd forgotten how much influence this paper has had on my thinking. Rereading
   it now, I'm seeing that a large portion of my meandering theories are just
   attempts to rephrase and understand his basic idea of reflection.

   For instance, Smith's equation relating denotation to operation in lisp:

   ∀ s ∈ S, if ϕ(s) ∈ S then ψ(s) = ϕ(s) else ϕ(ψ(s)) = ϕ(s)

   Is exactly what I've been calling "generalised homoiconicity".

   It says, loosely, that if a form denotes a form, then the interpretation of
   the form *is* its meaning. Otherwise the meaning of the form is the meaning
   of its interpretation.

   Hickey's emphasis on making literal data syntactically explicit actually
   makes the equation above much easier to understand. I don't think I would
   ever have seen the significance without having programmed in clojure.

   It shouldn't be surprising that my ideas aren't original. Ideas are never
   fully original. Now that I've remembered where these originate, I have some
   reading to do:

   - [ ] A Simple Reflective Interpreter, Jefferson & Friedman 1996
   - [ ] Intensions and Extensions in a Reflective Tower, Danvy & Malmkjær 1988
   - [ ] The Mystery of the Tower Revealed, Wand & Friedman 1988
   - [ ] The Theory of Fexprs is Trivial, Wand 1998
     I've read the intro of this paper before and took it as a dismissal fexprs
     and mathematically uninteresting, which I believe is the general takaway in
     the community.

     But doesn't that mean that we have a solution to function equality? That is
     to say, does this paper actually prove that a lisp interpreter can be
     qualitatively more powerful than the lambda calculus? I definitely need to
     read this now. The implications for robust engineering are impossible to
     overstate if that's the case, even if the math is boring.
** [2022-10-07 Fri 12:00] More Reflection on Reflection and Semantics in Lisp
   At the end of section 7, Smith writes "It is noteworthy that no reflective
   proceedures need to be primitive; even LAMBDA can be built up from scratch."

   Here's the implementation of λ:

   (define lambda
     (lambda reflect [[kind pattern body] env cont]
       (cont (ccons kind ↑env pattern body))))

   So all lambdas are defined in terms of lambda reflect. That's really cool,
   but we have a bootstrapping problem: lambda reflect needs to be built in
   before lambda can be defined. Isn't that a necessary reflective primitive?

   Need to read Smith and des Rivières 1984 to see how they break the cycle.

   Does he not consider bootstrapped circuit breakers to be primitive, or am I
   missing something?

   The initial lambda implementation is very important since it's an opening
   for Thompson quines.

   But beyond security considerations, it's that circuit breaking kludge that
   shows the lie of lisp, by itself, as a full theory of computing
   machinery. Something else needs to exist for a lisp to be built on top of,
   and how lisp is implemented in that something else determines the ultimate
   reach.

   So what if instead of having an initial lambda in terms of which lambda is
   defined, we had a call down to a lower level which explicitely says
   "`lambda` at the lisp level is defined in terms of `lambda` in the
   substrate."? What is the substrate? That's an implementation concern, but it
   could be anything from raw hex up to clojure, it depends on what the
   language is implemented in.

   Or perhaps, more concisely, it depends on the interpreter of the interpreter
   that we call "lisp".

   The tower can be arbitrarily high, but it goes down to the hardware and ends
   there always. How high it goes depends on how much reflection an application
   needs, and how far below on what tech stack is used to build it.

   The "programming language" is always in the middle of a tower. If the
   language is sufficiently expressive we build up from the language to
   something higher, but even the least expressive of languages are implemented
   in something else all the way down to machine code, or microcode, or verilog
   and fpga layout, depending on how far you want to look.

   The height of stacks nowadays is often lamented as a problem. Languages like
   go and rust which compile right to machine code are one way of getting
   around that problem, but they do it by restricting how high the programmer
   can climb (because the compiler has to understand everything top to bottom
   and that's just too hard in general for any program we can currently
   write).

   I'm thinking the opposite. Allow the stack to grow as high as necessary to
   express the program you want to write as cleanly as possible. Simple,
   obviously correct programs sitting on top of many layers of progressively
   more complex but tractible abstractions. But keep the stack explicit. The
   tower of technologies is invisible to the programmer who doesn't care, but
   is always available for inspection, debugging, tooling, or optimising.

   After all, once you have the simple and elegant solution, the best way to
   optimise it is to quash the inner layers of abstraction while preserving the
   simple surface.

   Ultimately, even though 3-lisp defines lambda as a userspace function, the
   meaning and behaviour of that function will always depend on the behaviour
   of an invisible kludge that was shoved in to get it all started and then
   deleted and forgotten about.
