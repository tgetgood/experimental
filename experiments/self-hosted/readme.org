This is a scratchpad of what might feel like to program in xprl.

It's rapidly becoming yeat another attempt to organise a sprawling spiral of
research. That's okay; I really don't know where I want to take this yet.

Initially I'm writing the language in itself to make sure it's sufficiently
thought out to do something useful.

Bootstrapping is going to be fun.

* Reading List
   - [X] Lambda the Ultimate Declarative — Steele 1976
     | It is not the names which are important to the computation, but rather    |
     | the quantities; hence it is appropriate to focus on the quantities and    |
     | think of them as having one or more names over time, rather than thinking |
     | of a name as having one or more values over time.                         |

     From section 1.3. I don't know if I've ever seen that point of view so
     clearly stated. And in 1976...

     The argument by induction (section 1.5) that no lambda expression ever
     actually pops the control stack strikes me as profound.

     Only primitives ever pop return addresses off the control stack in a
     functionally implemented language. To convert that language to
     continuation passing style, it suffices to convert just the
     primitives. Except for those lambdas that now want to manipulate control
     flow...

     So we only need to focus on the primitives. But what are the primitives?
     Primitives are *other interpreters* to whom we send messages. Furthermore
     those other interpreters are defined by code and the interpreter of the
     other interpreter's code.

     This is a tower that ends up reaching down to machine code for each
     primitive. That sounds like a lot of work, but isn't that just what
     implementing a language is?

     This focus on interpreters all the way down means that we need contracts
     of some sort to be enforced. After all, what use is it knowing that `+` is
     an interpreter whose code is X and whose parent interpreter is `lli`, if
     we can't use that information to replace `+` with an interpreter whose
     code is JIT compiled and optimised? Or with code that can be passed to a
     parent interpreter on a different platform which doesn't support the
     current parent?

     I truly feel like I'm making progress, but this is certainly taking me in
     a spiral.
   - [X] Lambda: The Ultimate Imperative
   - [ ] Actors: A Model of Concurrent Computation in Distributed Systems
   - [X] Intensions and Extensions in a Reflective Tower, Danvy & Malmkjær 1988
   - [X] The Mystery of the Tower Revealed, Wand & Friedman 1988
   - [X] The Theory of Fexprs is Trivial, Wand 1998
     I've read the intro of this paper before and took it as a dismissal fexprs
     and mathematically uninteresting, which I believe is the general takaway in
     the community.

     But doesn't that mean that we have a solution to function equality? That is
     to say, does this paper actually prove that a lisp interpreter can be
     qualitatively more powerful than the lambda calculus? I definitely need to
     read this now. The implications for robust engineering are impossible to
     overstate if that's the case, even if the math is boring.
     After rereading sections of this paper, I still don't understand the details of
     the argument, but the meaning seems more relevant. It has to do with optimising
     compilers and source rewriting: if context is so important that only identical
     things (in context) are equal, then the compiler has to have insight into the
     semantics of the program because purely syntactic transformations are
     impossible.

     This seems to be more because sytax and semantics cease to be separate domains
     that can be operated upon individually. Thus the entire formailst ediface is
     inapplicable and so on and so on.

     That doesn't seem so bad. But it does mean that we need a new way of
     expressing the meaning of a program.
   - [X] Mathematical Foundations of Joy
     Denotational semantics is such an unsatisfying interpretation of
     meaning. Syntax and semantics are strictly separated
     because.... because. Computers are formal symbol manipulators, which is
     another way of saying they're purely syntactic.

     So syntax is an algebraic construct of some sort (a monoid in Joy's case)
     and we want to relate it to semantics. So why don't we find a homomorphism
     of some sort from our syntactic algebra to some other algebra and call that
     other algebra the semantics?

     Okay, awesome. But what the hell does it mean?

     We've lost the point. What does it mean to say "A means B"?

     What if one tried to build a language semantics on Bateson's slash?

     Semantics aside, the notion that every word is a function from stack to
     stack (or in a more general sense from context of evaluation to context of
     evaluation) is a fascinating perspective. Om takes this even further by
     viewing every word as a function from continuation to continuation.

     What if eval only operated on the environment? That env moving around the
     system is in a real sense the entire computation up to this point. We can't
     know what inputs are going to come in (think repl development), but we
     always know to what that new code will be applied.
   - [X] Reification: Reflection without Metaphysics; Friedman, Wand 1984
   - [ ] Brian Smith's Phd Thesis
     This thing is 762 pages long. That's just crazy. But judging by the table
     of contents, the first 150 pages or so are likely to be full of useful
     insight.
   - [ ] Reread The Early History of Smalltalk
   - [ ] Art of the Metaobject Protocol
     You know a book is above your paygrade when you read it and think "Hey they
     didn't do anything." and then realise that they've implemented an entire
     language without you noticing.
   - [X] Guarded Commands, Nondeterminacy, and Formal Derivations of Programs
     This is a very interesting idea, and I see why Hoare chose it as one of
     the foundational constructs of CSP.

     Nondeterminism was considered to be such and important thing in the past,
     but I can't help but think of it as basic confusion between knowledge
     (information) as an ontological vs epistemological topic.

     The guarded if construct is very similar to squiggol filter/remove pair as
     a branching construct. Just let both threads of execution run in parallel
     and know that only one will make it past.

     More interestingly, what if we replaced both kinds of guarded statement
     with a single guarded expression which takes a map from predicates to
     continuations and continues on each fibre whose predicate is true
     (concurrently). You need strict immutability to make sense of anything in
     this situation. But is that sufficient?

     I should extend Dijkstra's calculus and try to construct a program from
     it. That would at least give me a better intuition of what he's trying to
     do.
   - [X] Communicating Sequential Processes — Hoare 1978
     I've read this before in the distant past. Time I took another look at it.

     The first thing that jumps to mind is the lack of channels. Processes
     communicate directly by knowing each other's names. That means that for A
     to pass a message to B, not only must A know B's name (address, whatever),
     but B must know A's name, and the two must be waiting for each other at
     exactly the same time.

     That's a highly artificial situation. The whole fabric is tightly coupled
     (more like bound or braided) since everything must be aware of everything
     else for anything to work.

     The guards and coroutining lead to very elegant programs to solve problems
     involving IO that only have ugly solutions in most languages. That's an
     accomplishment.

     Are multiple entry and multiple exit good things? They mostly seem to be
     attempts to avoid function calls when possible.

     Ultimately this paper is an exploration on the lines of The Art of the
     Interpreter. It isn't a language, but a rough sketch and a lot of examples
     of how you might solve real problems with the sketched solution.
   - [X] The semantics of a simple language for parallel programming — Kahn 1972
     This is an interesting take on things. It has more in common with what's
     called CSP nowadays than do Hoare's early writings.

     The main difference is that processes communicate over buffered channels
     and in general to not stop.

     I think that Hoare's concern about OSes that don't clean up and exit when
     told to do so is misguided. Yes, a long running program should exit when
     told to do so, but when it halts is completely independent of what it's
     computing. Nothing within the program should ever cause it to halt, only
     the signal coming from the operator outside. That's not halting in the TM
     sense.

     There's also a robustness question. Systems will crash, and so should be
     designed to do so gracefully, without losing anything. Erlang manages this
     with supervisor hierarchies. Kahn's approach is more academic and doesn't
     deal with failure and recovery, but he works for a nuclear power
     authority, so I think it's safe to say that safety and recovering from
     failure are merely excluded from this particular treatment of the
     problem.

     Now to the paper itself. Kahn does not deal with non-determinism. Looking
     back at a computation that has been running, you will see a definite set
     of messages have been passed over each channel. You don't, a priori, know
     what those messages will be — aside from their types in this case — but
     they aren't non-deterministic. Each message was sent by a process, a
     peripheral device, or a human. Each message was sent for a purpose and in
     response to other messages. These causes are too complex to analyse
     meaningfully, but pretending that there are no causes does not help us in
     any way. The order of the messages depends on the topology of the network,
     the properties of cables, noise, etc., and again taking a quantum mindset
     that "we don't know and thus god can't know." gains us nothing.

     So, looking backwards we will see a definite sequence of messages on every
     channel. How can we constrain the processes so as to maintain mathematical
     invariants on these sequences of messages even when we don't know what
     they will be?

     Working with the lattice of preficies of sequences is natural, but I
     hadn't thought of it. The primary goal is always to constraint a sequence
     based on our knowledge of a prefix of it.

     The functional (or applicative) bias of using one function per output
     seems overly complicated. We could as well use a single relation on the
     combined set of inputs and outputs. But it remains to be seen what we can
     infer from that. The author points out the presence of the relation, but
     the idea of using relations to run computations backwards is far too new
     for this paper to have addressed.

     The induction proofs given only work if a finite buffering capacity is
     sufficient. If too much data must be buffered and backpressure forces
     producers to park until their output is needed, then the whole system can
     easily end up in deadlock.

     But over all, this seems like a wonderful way for a compiler to prove and
     optimise programs. It seems it would be quite expensive, but in the
     immutable context necessary for the proofs to make any sense, there's no
     reason to ever work out a proof more than once (sign and publish it to a
     repository, checking a proof is easier than coming up with it).

     An early mention of the idea of lazy producers comes up as implementation
     advice at the end of section 4.

     Equivalence of schemata is decidable, independently of whether the
     schemata in question are recursive or not.

     That either makes these machines equivalent to primitive recursive
     functions, or something else entirely. I suspect the former given the
     strength of the theorems presented, but I don't know yet.

     | A *good* concept is one that is closed
     | 1. under arbitrary composition
     | 2. under recursion

     I like that advice. Now who is Scott? Possibly Dana Scott: "Outline of a
     Mathematical Theory of Computation", 1970.
* Languages to learn more about
  - Factor
  - Joy
  - See where Unison has gotten
  - Minikanren and relational programming in general
  - CLOS
  - Go
    Comparatively boring choice? Well people who work in go day to day always
    tell me the same thing: go is boring and yet incredibly productive. They get
    everything done and go home early on a regular basis.

    That is the most understated attestation of excellence I've ever heard. I
    want to see it in action. Cool is only useful to draw the crowd.
  - Multilisp
    Multilisp had a single concurrency primitive `pcall`.

    `(pcall f a b c)` is like (call f a b c) — lisp-2 calling convention —
    except the evaluation of the arguments `a`, `b`, and `c` happens in
    parallel.

    That's it. That's enough to get a decent amount of parallelism in lisp since
    function application is pretty much the entire language.

    But what if we automatically parallelise transducers (map, filter, etc.)?
    That would give us the same effect for function calls since (eval '(f a b
    c)) => (apply (eval f) (map eval '(a b c))), but it would also give us nice
    optimisation for streaming computations. Choosing between simd, multiple
    parallel coroutines, and gpu vector comp could then be a runtime
    optimisation in the same location.

    Just a thought.
* Questions
** [2022-09-28 Wed 12:26] def, intern, and purity
   If there are no side effects, how does one implement `def`?

   You don't. We need side effects somewhere, but they have to be constrained to
   the communication layer.

   I think of the communication layer as a hypergraph (though I keep coming back
   to the idea of using symplectic topology to analyse it, so maybe simplicial
   complexes are a better foundation...) where the edges are the emission
   channels (one writer, potentially many readers) and the nodes are either

   1) Pure computations which commence when a message is available on each input
      channel and terminate with a map from channels to lists of messages.
   2) Sources, which take no input, but emit (potentially infinitely many)
      messages to their output channels.
   3) Sinks, which receive messages but emit nothing.

   Sources and sinks are the edge conditions of the system. Sources allow
   repeatable interaction with things like time, PRNGs, etc. by logging the
   messages.

   Sinks, on the otherhand are the escape valve that lets us do anything we have
   to do. Sinks have to able to do anything, otherwise we can't implement the
   language, but they also need to be heavily restricted most of the time,
   otherwise we'll never be able to understand what a program might do.

   To implement `intern`, we would need a sink/source pair where the sink
   receives messages saying "merge this form into the trie", and the source
   emits messages saying "Ref has been merged into tree". The actual magic lives
   in the gap between sink and source.

   Sending messages over a network is the same sort of proposition. We need a
   sink that takes request data, creates sources which will eventually emit
   reponse data (or errors), sends those new sources somewhere, then sends the
   request and sets up the response listeners.

   It seems painfully intricate and potentially a point of failure. But I hope
   that pushing these details to the edge of the system will make the centre
   much easier to manipulate and reason about. Time will tell.

** [2022-09-28 Wed 12:42] Multimethods and static linking
   The biggest failing point of multimethods, in my experience, is that they are
   global mutable variables, so suddenly the behaviour of your program depends
   upon the order in which code modules get loaded.

   Ultimately it's unavoidable that the compiler has to know about the code you
   want to call before it can emit the code for the call.

   My solution (at present) is to make it so that polymorphism is restricted to
   the set of methods known to the reader when the code making the recursive
   call is read. That way the developer can inspect the set of possible methods
   (fixed), and make sure the one they expect is present. The actual dispatch
   still happens at runtime, but the choices are fixed at dev time. Incidentally
   it should also be possible for the developer to add annotations reducing the
   size of the set of possible implementations to 1, thus ensuring the jit will
   insert a direct call, when that's needed.

   The two layers of buzzpop should make this simple to implement. Every
   concrete method is interned in the form trie, but when a name is overridden,
   one of two things must happen.

   1) If the name is known to be a simple indirection, then the name trie gets
      updated, and you need to use time travel to find what the name used to be
      for things read in in the past.
   2) If the old and new versions of the name point to indirect indirections,
      then we can merge those indirect indirections. Note that the trie is still
      updated with history so that previous versions of the dispatch table can
      be referred to. This allows one symbol to point to different sets of
      methods depending on the relative points at which the references and
      definitions of that symbol are read.

   That sounds absurdly complicated. And it is. But that complication is
   inherent in the problem of building an intertwingled dynamic system by
   linearly scanning source files.

   One of my core goals is to prevent the programmer from being able to lie to
   themselves about what they do and do not know.

** [2022-10-06 Thu 09:19] Context and fexprs
   The most common issue I've been having with a complete lack of side effects
   is the maintenance of local state. The language itself needs to keep internal
   state so that new defs can be referred to later on.

   Modelling state as function sending results back to two locations is a
   kludge. It's not that dissimilar to the state monad in that it keeps state
   hidden away inside some secret loop that isn't readily accessible except when
   necessary.

   That's the wrong way to go about it entirely.

** [2022-10-06 Thu 10:23] Reflection and Semantics in Lisp
   Brian Cantwell Smith 1984

   I'd forgotten how much influence this paper has had on my thinking. Rereading
   it now, I'm seeing that a large portion of my meandering theories are just
   attempts to rephrase and understand his basic idea of reflection.

   For instance, Smith's equation relating denotation to operation in lisp:

   ∀ s ∈ S, if ϕ(s) ∈ S then ψ(s) = ϕ(s) else ϕ(ψ(s)) = ϕ(s)

   Is exactly what I've been calling "generalised homoiconicity".

   It says, loosely, that if a form denotes a form, then the interpretation of
   the form *is* its meaning. Otherwise the meaning of the form is the meaning
   of its interpretation.

   Hickey's emphasis on making literal data syntactically explicit actually
   makes the equation above much easier to understand. I don't think I would
   ever have seen the significance without having programmed in clojure.

   It shouldn't be surprising that my ideas aren't original. Ideas are never
   fully original. Now that I've remembered where these originate, I have some
   reading to do:

   - [ ] A Simple Reflective Interpreter, Jefferson & Friedman 1996
   - [ ] Intensions and Extensions in a Reflective Tower, Danvy & Malmkjær 1988
   - [ ] The Mystery of the Tower Revealed, Wand & Friedman 1988
   - [ ] The Theory of Fexprs is Trivial, Wand 1998
     I've read the intro of this paper before and took it as a dismissal fexprs
     and mathematically uninteresting, which I believe is the general takaway in
     the community.

     But doesn't that mean that we have a solution to function equality? That is
     to say, does this paper actually prove that a lisp interpreter can be
     qualitatively more powerful than the lambda calculus? I definitely need to
     read this now. The implications for robust engineering are impossible to
     overstate if that's the case, even if the math is boring.
** [2022-10-07 Fri 12:00] More Reflection on Reflection and Semantics in Lisp
   At the end of section 7, Smith writes "It is noteworthy that no reflective
   proceedures need to be primitive; even LAMBDA can be built up from scratch."

   Here's the implementation of λ:

   (define lambda
     (lambda reflect [[kind pattern body] env cont]
       (cont (ccons kind ↑env pattern body))))

   So all lambdas are defined in terms of lambda reflect. That's really cool,
   but we have a bootstrapping problem: lambda reflect needs to be built in
   before lambda can be defined. Isn't that a necessary reflective primitive?

   Need to read Smith and des Rivières 1984 to see how they break the cycle.

   Does he not consider bootstrapped circuit breakers to be primitive, or am I
   missing something?

   The initial lambda implementation is very important since it's an opening
   for Thompson quines.

   But beyond security considerations, it's that circuit breaking kludge that
   shows the lie of lisp, by itself, as a full theory of computing
   machinery. Something else needs to exist for a lisp to be built on top of,
   and how lisp is implemented in that something else determines the ultimate
   reach.

   So what if instead of having an initial lambda in terms of which lambda is
   defined, we had a call down to a lower level which explicitely says
   "`lambda` at the lisp level is defined in terms of `lambda` in the
   substrate."? What is the substrate? That's an implementation concern, but it
   could be anything from raw hex up to clojure, it depends on what the
   language is implemented in.

   Or perhaps, more concisely, it depends on the interpreter of the interpreter
   that we call "lisp".

   The tower can be arbitrarily high, but it goes down to the hardware and ends
   there always. How high it goes depends on how much reflection an application
   needs, and how far below on what tech stack is used to build it.

   The "programming language" is always in the middle of a tower. If the
   language is sufficiently expressive we build up from the language to
   something higher, but even the least expressive of languages are implemented
   in something else all the way down to machine code, or microcode, or verilog
   and fpga layout, depending on how far you want to look.

   The height of stacks nowadays is often lamented as a problem. Languages like
   go and rust which compile right to machine code are one way of getting
   around that problem, but they do it by restricting how high the programmer
   can climb (because the compiler has to understand everything top to bottom
   and that's just too hard in general for any program we can currently
   write).

   I'm thinking the opposite. Allow the stack to grow as high as necessary to
   express the program you want to write as cleanly as possible. Simple,
   obviously correct programs sitting on top of many layers of progressively
   more complex but tractible abstractions. But keep the stack explicit. The
   tower of technologies is invisible to the programmer who doesn't care, but
   is always available for inspection, debugging, tooling, or optimising.

   After all, once you have the simple and elegant solution, the best way to
   optimise it is to quash the inner layers of abstraction while preserving the
   simple surface.

   Ultimately, even though 3-lisp defines lambda as a userspace function, the
   meaning and behaviour of that function will always depend on the behaviour
   of an invisible kludge that was shoved in to get it all started and then
   deleted and forgotten about.
** [2022-10-13 Thu 10:43] Reflection without infinity
   The approach of Friedman and Wand is intriguing, but the `meaning` builtin
   seems like a mistake. The builtin "spin up a new interpreter and run code
   there (using this interpreter to interpret that interpreter)" is a clever
   hack to avoid the infinite tower. Something similar, though poorly formed,
   occured to me when reading Smith's paper in the first place.

   But do we really need that `meaning` operator. And perhaps more importantly,
   do we want to spin up a copy of the *same* interpreter, or give the user the
   ability to define new interpreters at will and embed them within the code?

   Take the macro definition in fexpr.xprl (as of now). What we have is a sort
   of meta evaluation protocol. `eval` dispatches on the type of its
   argument. Lists being the primary metaphor for passing information around in
   lisp, `(eval ^List ...)` invokes `apply` which is where the bulk of lisp
   happens.

   But `eval` passes on expressions without evaluating them, and `apply` itself
   dispatches on type — and I'm allowing specification to instances in this case
   (though I ought to namespace qualify everything in advance) — the combined
   effect being that I can specialise `apply` to the symbol `xprl.core/fn` and
   have apply create a datastructure representing a function declaration (*not*
   a compiled proceedure). Then when `apply` is called on one of these function
   objects, it does what you expect (evals the args, binds the results to the
   function arguments, and then evaluates the result). But we can dispatch
   `apply` on the symbol `xprl.core/macro` as well, creating a different kind of
   datastructure (which is really the same as a function object but of a
   different type) and then not evaluate the arguments passed to a macro
   object.

   `xprl.core/fn` and `xprl.core/macro` are effectively keywords since they're
   defined as specialisations of the interpreter itself. But the user can define
   new keywords in the same way freely. They can even define their own
   interpreter and completely replace the builtin one (though the builtin one
   will be interpreting it, which maybe I can avoid).

   But back to reflection. When the programmer is able to define new *expression
   types* they can control what is evaluated when and thus manipulate the
   datastructures that are going to be evaluated before passing them to
   eval. That's reflection. If the user wants to manipulate the new expression
   type before its version of eval/apply gets invoked, they can define yet
   another expression type and indirect evaluation another level. And so on ad
   nauseum.

   I've never seen any practical use in using reflection more than 2 levels
   deep, but maybe I just haven't been looking. With this metaeval protocol we
   can reflect as deeply as we need to, but we have to do the work of setting up
   each new level as we go. More work, less magic. I think that's a good trade.

   Going back to a second to the idea of lists being the metaphor for message
   passing in lisp: a list is considered to be the implicit invocation of a
   function with arguments. Or seen from a message passing point of view, a list
   is a specification '(f & args) that says "send the message `args` to `f`
   (after interpreting what is meant by args) and wait for a response."

   With continuations we don't need to worry about waiting for responses since f
   will either yield to some process of its own accord, or yield back to the
   current continuation (as a return value), but we don't care, that's up to f.

   So what if `f` is another interpreter? and especially what if it's another
   interpreter messaged from tail position that never returns to the calling
   interpreter? This is the sort of setup I'm looking for to bootstrap higher
   level languages that are capable of stepping down to lower layers for interop
   purposes.

   It's also a way to have a conceptually deep stack with lots of descriptive
   layers that aid development and debugging, but can be stripped away entirely
   when performance is the main concern.

   But I'm getting way ahead of myself.
** [2022-10-18 Tue 10:25] Microthreading
    Cf http://subdivi.de/~helmut/libmuth/tutorial.html,
    https://en.wikipedia.org/wiki/Micro-thread_(multi-core)

    I'm finding a glut of resources on concurrency, and basically nothing on
    parallelism. Continuations can build any concurrent construct (as far as
    I've seen mentioned), but what about constructs (either theory or patterns)
    to run many continuations in parralel?

    There are resources like Goetz's book on concurrency in java, that contain a
    lot of practical advice on how to build reliable software based on an
    adversarial thread model.

    But not a lot on new models. Data parallelism, task parallelism, actors,
    CSP,... that's pretty much it.

    Microthreads, as above (I have no idea what libmuth was, but the tutorial is
    short and well written) are just functions intended to be run in
    parallel. But the model is still threading, with semaphores and the
    rest. The main innovation in the Cell archtectures mentioned is extending
    the ISA to make microtaskswitching more efficient. That might be an
    improvement of the von Neumann bottleneck, but our current hardware is still
    going to dead end with the scorpion.

    But there's nothing here that wasn't in erlang. Run one scheduler per core
    and keep a queue of coroutines (or whatever you want to call them). It seems
    like the state of the art hasn't moved since I was born. Or it might be
    better to say it's been going in circles.

    I can squeeze what I'm building into these paradigms: Program execution
    generates a stream of thunks which get enqueued and multiple schedulers (one
    per core) pop thunks off the queue and run them. Data dependencies are
    generally sorted out because you don't have a thunk until you have all the
    data. One thunk can generate data and pass it on to a continuation which
    uses it to make (and enqueue) another thunk.

    It's not very pretty, but I think it works. Implementation will tell where
    the weakest parts of the hand waving lie.
** [2022-11-06 Sun 10:41] Are Exceptions Necessary?
   Section 3.2 of Lambda the Ultimate Imperative brings up an interesting
   point. Well, exceptions are nothing new nowadays (though they were somewhat
   new back then), but reading about them in this context — especially the name
   "escapes" — gave me an idea:

   The exception in the example, and many exceptions in my experience, are
   really just a way to interleave a question about a whole collection with a
   computation iterated over its elements. In this case we first ask "is any
   element zero?" and if so just return zero, if no elements are zero, we
   proceed to calculate the harmonic mean. So something like:

   (if (any? zero? coll) 0 (/ (count coll) (reduce + (map reciprocal coll))))

   The code above is much cleaner, its intention is clear, it's so simple as to
   be obviously correct. The cost we pay for that simplicity is scanning the
   whole collection twice, which depending on the size of the collection might
   be an unreasonable thing to ask.

   But what about loop fusion? Escapes (exceptions) provide a way check a
   predicate as you go and abort the running computation, jumping to some other
   computation instead. They provide a manual mechanism by which to fuse the
   loops, but can't we do this automatically?

   In this trivial example it's not difficult to cook up several ways of
   accomplishing it, but in general I doubt it will be simple. Still, it's
   worth pursuing.

   The implications are also worth investigating. In a language with no side
   effects, an intensional (non applicative) `if` is unnecessary. For the sake
   of performance we don't, in general, want to compute a value that won't be
   used, but there's no harm in doing so because simply running a computation
   can't effect the world in any way. We have to be careful about transmitting
   messages out of the if, since sending messages to both continuations would
   violate the meaning of "if", but computing, in itself, is fine.

   This brings us back to the holarchic structure. The consequents
   (continuations, branches, whatever) of the `if` are holons. They are wholes
   unto themselves and can create sub structures and freely pass messages to,
   from, and between those substructures. But the consequents cannot send
   messages to anything they did not create. They cannot effect the outside
   world except via channels that were passed to them when they were
   created. Thus the `if` controls how its consequents may and may not talk to
   the outside world. Consequently, the `if` might run both consequents, and
   the predicate, all in parallel, connecting their output channels to buffers
   that for the moment go nowhere, and only when the `if` has a firm answer
   from the predicate, does it connect the buffer from the appropriate
   consequent to its own output channels.

   Thus can we, in principle, safely fuse the iteration of the predicate with
   the iteration of the consequents (could we conceivably fuse all three
   safely? that's a cool question).

   There are other uses of exceptions. Propagation of errors can probably be
   handled by the "STDERR" channels that are implicitly pass everywhere, though
   the ergonomics are as yet a mystery. Creating tail recursion where there was
   none previously (Akka) is an ingenious kludge, but we don't need it.

   Exceptions are just controlled GOTOs that we tolerate because they're
   "exceptional", so I'm not going to cover all of their possible uses. But
   that's not a problem.
