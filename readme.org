#+TITLE: Experiments in Language Design

This repo is a bucket for experiments and prototypes,which may not on the
surface appear to be related at all. I think they are, but that might change.

#+TOC: headlines 2

* An Informal Theory of Computation
** What is this?
   What is the purpose of xprl? What are the assumptions underlying its design?
   What's the theory of computation underlying its programming model? What
   problems is it intended to solve? What novelty does it introduce? Why do I
   bother putting so much time into organising these fractured thoughts?

   These are all hard questions and I don't have answers as of yet. I'm thus
   forced to resort to a collage of sorts...
*** Perspective: Coordination Languages
    Cf https://dl.acm.org/doi/10.1145/129630.129635

    The basic idea of a coordination language is that computers do two
    orthogonal things: they compute, and they communicate. Thus there should be
    two levels of design (and possibly two separate languages) in
    programming. One to define the computations and the other to specify how
    they communicate with one another.

    The universal bubble (lambda expressible, turing computable, recursively
    definable, &c.) is a mathematics of closed formal systems. They express the
    notion of computation, but by being closed they proscribe communication.

    In the same paper in which Turing defined his automatic machines (1936) he
    also defined a class of machines which he called choice machines which
    communicate with a human operator. He showed that choice machines, by being
    able to ask an arbitrary number of yes/no questions are qualitatively more
    powerful than automatic machines (which we today call Turing machines). The
    gist of the argument is that the set of Turing machines is countable,
    whereas the set of choice machines is not.

    It shouldn't be controversial that Turing universal is a small subset of
    "can be done by modern electronic devices" — it was clearly stated by Turing
    before any such devices existed — but I've spent too much of my life going
    over this in bars late into the night. Read the Standford encyclopedia of
    philosophy entry on the Church-Turing thesis.

    So if Turing machines are very limited, why use them? The answer is because
    we have very powerful mathematical tools for creating, testing, proving, and
    thinking about them. Our knowledge of communication is very poor in
    comparison. Except when it comes to telephony.

    Hypothesis: We can build a pure functional language that has no side effects
    — no I/O, no mutable references, &c. — except for a single primitive `emit`
    which sends a bundle of messages and can only be called as the final step of
    a computation. We can then reconstruct a practical language for real
    software by using a DSL to wire together these pure units into networks
    (hypergraphs? simplicial complexes?).

    Conjecture: If such a language can be built, then it can be built from
    primitive recursive units wired into networks. Branching and recursion can
    be expressed as message passing topologies, so we can build general
    recursive functions without choice. This also has practical significance
    because primitive recursive functions are easy to prove correct (induction)
    and always halt if correct.
*** Perspective: Special Relativity
    - Note taken on [2022-09-24 Sat 18:47] \\
      Originally dates [2020-05-12 Tue] in my notes
    On an interstellar scale, consensus is not possible in any practical
    fashion.

    This is a trivial consequnece of special relativity. It is not only
    possible, but necessary, that different observers will observe different
    sequences of events. Those sequences will often contradict each other in the
    short term, but both observers will nevertheless be correct.

    So what can we do? Current solutions like paxos, raft, etc. work by builing
    consensus via coordinated communication. They guarantee that eventually
    everyone will agree, but they make no promises (because they can't) about
    how long it will take for such consensus to arise.

    Spanner, and other "reengineer the universe" style solutions build a frame
    of coordinates that spans the entire planet and using atomic clocks and gps
    impose a total order on all events happening on the earth. Again, this is a
    form of consensus in the long run, with a blind spot trailing ever so
    slightly into the past. If you're running a server in Singapore, and you are
    sent messages from Sydney and London at about the same time (though
    according to the atomic clocks, the London message was first) you'll
    probably receive the packet from Sydney first, and so there will be an
    interval of time in which you are inconsistent with the total ordering of
    the system simply because messages travel at finite speed.

    On the earth this isn't much of a problem, since that trailing blind spot is
    from about a second ago to now. That's not much time. But what happens if we
    expand the network to include the moon? Mars? The moon is over a second away
    at the speed of light. Mars can be more than twenty minutes away. That means
    that the trailing blind spot will be at least 45 minutes for a network
    spanning the earth and mars if it only takes one round trip to agree to
    things. If you have a spanner like system, you can broadcast and get best
    effort consistency in 20 or so minutes, but you won't know they know until
    at least one round trip.

    Let's make it harder. Imagine we send a ship to another star. If that star
    is 60 light years away, then as the ship travels, round trip time will
    increase until it reaches 120 years. If you won't get acknowledgement of
    delivery for over 100 years, then you may as well never get it. Concensus
    becomes completely impossible, all that can be done is informing.

    So instead of tcp, we'll simply need to send a continuous stream of data and
    listen to one coming back.

    Now imagine this ship comes back. It's own web will have diverged
    significantly from ours, but because of the streaming updates back and forth
    there will be cross links. Once the two webs are brought physically close
    together it should be as if they were never apart. The network needs to be
    amorphous in this sense that pieces can break off, evolve in isolation, and
    then reconnect.

    If the network is robust — at the highest level of content, not just at the
    wiring level — to continuous changes in topology, then being connected all
    the time will become a less pressing concern. Going through a tunnel
    shouldn't break anything, being cut off from the outside world by censors
    should be equivalent to a lag in updates, instead of the current situation
    of only having access to anything when you find a hole in the firewall.

*** Interpretation all the way down
    | “To be is to do” —Socrates
    | “To do is to be” —Jean-Paul Sartre
    | “Do be do be do” —Frank Sinatra

**** Notes adapted from [2020-01-06 Mon] on paper
     Data is not a well defined term and I'm going to try and avoid using it for
     the moment. Is this possible?

     What is an inscription on a tablet? Let's call it a message for lack of a
     better word. How do we know it's a message and not a natural phenomenon? or
     an unlikely outcome of random noise? We don't. Intention comes right from
     the start: a message is something intended to be read. Intended to convey
     meaning. The author of a message meant something by it.

     So we need another concept for an artifact — an inscription, a shape, a
     sound, &c. — that *might* be a message, or might not. For now, let's call
     this a text. Not a good word, but I've got nothing else; signal, sign,
     etc. are loaded terms nowadays.

     Can anything be a message? If so, do we even need a word for something that
     might be one?

     A message only exists if a producer and a receiver share enough context to
     make communication possible.

     So before we can have messages, we need agents with the intent to
     communicate.

     Two sentient agents can communicate. This is an observation, not a
     definition.

     But what does it mean to communicate with a machine?

     If you send a message (from your point of view) to a machine and it behaves
     in an expected manner, then you can say that the machine interpreted your
     message correctly. Or maybe that with respect to that interpreter (machine),
     your message is code.

     A message that means something to someone is different from one which does
     not.

     We should likely relax this condition to say that if you send a message to a
     machine and it does *something*, then it interpreted your message (somehow).

     Correctness is not a notion applicable to communication in general.

     Is correctness essential to the notion of code? The rationalist tradition
     would say yes, after all the word 'code' orginally referred to a collection
     of laws in old French (though prior to that, codex just means 'book' in
     Latin).

     Most computer code is not correct in the sense of being not even wrong. So I
     think it's a mistake to equate computer code with formal systems of any kind
     when thinking of practice.

     What do we call a message that you can interpret meaningfully? Meaningful
     (to you)?

     It's important to consider that the meaning intended by the producer of a
     message, and the meaning interpreted by the receiver need not have anything
     to do with one another. In fact one end, or the other (or both?) might not
     assign any meaning at all.

     So Barthes put it too strong: the author isn't dead, they're just another
     reader (of their own message).
**** Theory
     Data is the medium in which computation can occur.

     A message which causes an interpreter to do something is code. That
     something might be a side effect, or it might be a computation.

     It's important to see that from this point of view, computation is just
     another kind of effect we can have on the world. It is a form of doing. It
     just has much nicer mathematical structure than most other kinds of effect.

     That which is done by an interpreter, given a message, *is*, in some sense,
     the meaning assigned to that message by that interpreter. Meaning is use.

     Literal data are precisely those messages which pass through an interpreter
     without causing it to do anything. Semantic roughage. Sort of. Think of the
     machine that draws letters in the sand. There is a clear isomorphism
     between input and output and so there is a sense in which unicode chars are
     literal data, but equally a sense in which they are code. This is a point
     for further consideration elsewhere.

     For now, let's just consider literal data as a subset of (digital) data.

     Notably, since literal data passes through an interpreter without causing it
     to do anything in particular, literal data is not code, and so the
     interpreter assigns no meaning to it.

     (Literal) Data is purely syntactic. Semantics are external to it.

     The fact of the existence of literal data says something about a shared
     structure between the data and the interpreter which emits it unchanged.

     Note that the meaning that the producer of code assigns to it does not
     necessarily have anything to do with the meaning assigned to it by a given
     interpreter (though it may be important to others).

     A symbol, say `identity`, is a message that refers to a form, that is, to
     another message `(fn [x] x)`.

     A form which is intended to invoke a function referred to by a symbol, say
     `(identity 1)`, indicates by the grammatical position of `identity` —
     invocation position — that `identity` is in turn to be treated as an
     interpreter.

     (eval (identity 1)) <=> (apply (eval identity) (map eval [1]))

     The repl is an interpreter that interprets some parts of messages as
     interpreters and other parts as messages to those interpreters.

     But this process must bottom out. At some level, messages must *be*
     interpreters.

     Put differently, messages, being data, are inert. S-expressions, being
     *literal data* to the lisp reader are, furthermore, devoid of
     meaning. Meaning is assigned to the sexps by `eval`, which is the most
     important interpreter in a lisp.

     Code by itself — be it a string, or a forest of sexps — does nothing, means
     nothing. It is inert. It must be interpreted.

     `eval` in lisp serves two distinct purposes. It provides the grammar of
     lisp, which is to say that it decides which forms are the be considered
     interpreters, and which messages, and it transforms inert code into an
     active interpreter.

     This initial spark is magical. It is the difference between computers and
     all media that came before.

     This vivification of inert code into a reactive mechanism is not akin to
     compilation. A compiler is, in principle, a pure function that transforms
     one representation of computer code (text, sexps, etc.) into another (byte
     code, machine code, et al.).

     Compilation is often part of the action which transforms text into a living
     process, but it's not the magic.

     I keep saying magic, but really it's rather banal. At some point, you reach
     circuits which interpret messages directly into physical activity. You don't
     need interpreters all the way down. It just looks that way to people raised
     in modern platforms like the web.

     Does `eval` have to be singular? Is there any reason to restrict the set of
     meta-interpreters, the set of language defining interpreters, to a single
     thing?

     Why can't a single runtime platform accept messages in any format, so long
     as those messages are tagged in some way such that the runtime can deduce
     how to interpret them?

     That's basically how linux deals with requests to execute a file, after
     all.
**** Practice
     What I'm calling an interpreter might better be called an executor, but
     interpretation has a meaning beyond Steele and Sussman's art.

     The difference between compilers and interpreters, for our purposes, is that
     compilers are pure functions, that is they are computations that operate in
     and on data, whereas interpreters take action based on their input. Real
     action in the (possibly digital) world.

     In particular, a compiler itself needs to be interpreted, it's just a
     program.

     This is obfuscated because compilers are generally executables, which just
     means that the hardware interprets them directly.
*** Perspective: A Generalisation of Homoiconicity
    Let's start with a parable. One based on a [[https://news.ycombinator.com/item?id=11939851][true story]], but reinterpreted to
    suit my needs.

*** Perspective: Linguistic Archaeology
    Or how to organise your information so that it won't be forgotten.

    Is Linear A information? Rather is a text in Linear A information?  No. Not
    according to information theory, in any case.

    Information revolves around two parties sending messages across a
    channel. Information is an emergent property of this setup whereby one party
    informs the other. This informing crucially depends on shared context, an
    establised message format.

    Information cannot exist unless two parties share a context and messages.

    If you are given a binary file, what can you do with it? look at the first few
    bytes, see if it looks like ASCII, unicode, some sort of format header, look for
    repeating delimiters that might indicate a sequence, or regular sized chunks
    that might indicate records. If you're sufficiently familiar with conventions
    and lucky you can probably reverse engineer the code.

    If you're given a chunk of machine code for an old mainframe that you've
    never heard of and of which none exist anymore, then you're shit out of
    luck.

    This task is essentially the same as that of an archaeologist tasked with
    deciphering a piece of ancient writing in Linear A. If the surrounding
    context were known, that is if one knew linear B, and cypro-minoan, egyptian,
    etc. — not just in the sense that we "know" some of these today, but in the
    sense that one knows English, French, or Russian — then deciphering a text in
    Linear A might not prove incredibly difficult. Context is everything.

    So what is on a tablet inscribed in linear A if it's not information? A
    *message* is inscribed on the tablet, but if no one knows how to read it,
    then the meaning is lost and no *information* can pass between the author and
    the reader across the intervening millenia.

    When we program we send messages to machines. These messages take the form of
    source code, but what is source code? Does it contain information? If so,
    who, or what, is informed?

    Let us call something that can execute code an interpreter (ignoring
    compilers, direct execution, etc.). If you send a message to an interpreter,
    and it does what you expected it to in response, then in some limited sense
    it understood you or "got the message". You successfully informed the
    interpreter. Let us say that with respect to this interpreter, your message
    was code.

    A piece of text, or a blob of binary, or a group of dark spots on an optical
    disk, are messages. Those messages can only be code in reference to a specific
    interpreter. There need not exist an interpreter. A text file full of random
    noise cannot be interpreted meaningfully. There is by definition (of
    martin-lof randomness) no information to be had in the file.

    At the same time, a given message might be different code to different
    interpreters; see whitespace and polyglot code.

    It's important to stress that a text file by itself is not code in this
    sense. We can recognise a clojure source file by the .clj extension and so we
    know to which interpreter to feed it, but that knowledge is communal and
    contextual and can be lost. Without the interpreter the message is just a
    message and can go the way of linear A.

    In fact even without the .clj extension, if the file has been misnamed, we
    can still recognise it as a clojure source file because we're familiar with
    the language. But in this case even if we feed it to clojure it won't work
    because the compiler won't recognise it. So here we have a source file which
    we recognise as code, but which the interpreter with respect to which we call
    it code cannot, in fact, interpret it.

    Of course we can ourselves execute the code (on paper, in principle) using
    the semantics of the language which we know. So the interpreter is an
    abstract "clojure" that exists only in our collective cultural context, and
    not the =clojure= program itself.

    And this brings us to the point where we can ask the real question: if
    written text is a message, and a message that can be interpreted is code
    (with respect to the interpreter), what then is data?

    We talk about data all the time. Code is data. Data driven design. Data is
    better than functions are better than macros. Homoiconicity gives lisp the
    power of arbitrary syntactic extension preciesely because macros operate on
    the source code as a data structure. These definitions contradict each
    other.

    Or do they?

    Let's take a closer look at homoiconicty. Homoiconicity at its simplest is
    the statement that the text in source files is isomorphic to the AST of the
    language in memory. This isomorphism is the lisp reader. Its inverse is the
    printer.

    The power of macros is that they operate on the AST, but they look and act as
    if they were acting directly on the textual source code itself. This illusion
    empowers programmers — who can only really "feel" their code in the textual
    form that our editors manipulate — to extend our intuitions into the compiler
    of the program, giving us further intuitive reach than is possible in a
    language without homiconic syntactic macros.

    So if macros let us extend our intuitions about (static) source code into the
    runtime, why do we eschew them as a community? Why are functions better than
    macros?

    To answer this, let's extend our notion of homoiconity. Remember that
    homoiconicty is an isomorphism across the reader. Another way of saying this
    is to say that (print (read x)) is x for any valid sexp.

    But we don't just read source code. The LISP reader by itself is not very
    useful. After we read source files, we evalutate the ensuing sexp data
    structure. To phrase it a little differently the output of the repl is
    (print (*eval* (read x))).

    To rephase again: the reader interprets a message (the source text) and the
    result is the sexp data structure which itself is a message. This second
    message is in turn interpreted by eval which is what we generally think of as
    the lisp language.

    Note that messages are no longer homoiconic across this chain of two
    interpreters. The text "(+ 2 2)" comes back as the text "4". Syntax quoting
    exists to disable evaluation so that we can work with homoiconicy across
    eval.

    There is, however one class of text which is still homoiconic in a broader
    sense. What are those messages in text for which (print (eval (read x))) is
    identical to x?

    These are precisely the structures we call "literal data" in clojure. Quoted
    forms are notably *not* literal data because the quoting is lost and a second
    pass through the cycle will evaluate the form.

    This generalised notion of homoiconicity gives us a hint as to the nature of
    data. Or at least a definition that reconciles the apparent contradiction
    above.

    Code whose interpretation is isomorphic to its underlying message is data.

    This tells us that code is data *with respect to the reader*, but not with
    respect to the language as a whole.

    data is better than functions are better than macros because data (with
    respect to the repl) lets us extend our intuitions about textual source into
    the runtime in the same way that macros allow us to extend it into compile
    time, and functions work on data with respect to the repl, but are
    themselves not data with respect to the repl (though they are data with
    respect to the reader which allows macros to act on functions as functions
    act on literal data).

    This feels like semantic nitpicking until we rephrase the fundamental problem
    of programming (to paraphrase Dijkstra) as "knowing what is going to happen
    at runtime — given that the actual code executing is written not by a human
    but by another program (a compiler) — in terms of the textual code that we
    actually write".

    Being able to manipulate (and understand) the physical execution via a true
    isopmorphism with the text we write gives us an unparalleled avenue of attack
    on this problem. Data driven design is a qualitative improvement in a way
    that transcends all the petty bickering about so called programming paradigms
    (which are at best ideologies and all too often dogmata).

    So code can be data, functions can be code, and data can be better than
    functions. You just have to realise that the meaning of the terms code and
    data is not well defined without reference to an interpreter.

    To confuse matters even more, a message can be code with respect to two
    interpreters but only be data with respect to one of them. And the
    interpretation of a message need have nothing to do with the intentions of
    the author (cf. Roland Barthes 1967).

    Something decidedly less black and white than Barthes seems necessary to
    really understand this.

    Barthes holds that to give full creative autonomy of interpretation to the
    reader, one must let the author die, but of course it's more subtle than
    that.

    The author creates a text and (presumably) intends it to have a meaning. The
    text is transmitted, the meaning is not. The reader gets the text and infers
    a meaning by reading it.

    Traditional literary theory holds that one should look to the life, opinions,
    actions, etc. of the author when reading a text so as to try and infer the
    author's intended meaning.

    Post modern reading involves reading the text in and of itself — an act which
    is of course impossible because you cannot read except from the context of
    your own life and consciousness — and let the meaning come as it will.

    I think that we have to be schitzophrenic about it. We need to maintain
    simultaneous opinions about what we think the author meant from the context
    of their life, and what the author meant (or to further confuse matters what
    the text itself "means") from the text alone. These opinions will, in
    general, contradict each other. In a sense both will be true, in another
    sense neither will be true.

    All we can hope to do is contrast the different readings and make a call in a
    given context.

    — Aside on self description and indefinite archiving —

    Self description in this framework is ill defined. Description implies
    communication which is only possible through shared context. So you would
    need either a universal context, which is impossible, or a message combined
    with an interpreter that is capable of building a context in which to
    communicate from scratch. A feat which may or may not be possible. Maybe
    scratch isn't necessary, maybe lincos was onto something.

    Maintaining contextual consistency through the ages allows archaeologists to
    bring dead cultures back in a somewhat hollow form. To really understand a
    message, you need to keep a body of native speakers — or contextual natives —
    around. I don't know how long that can be possible.
**** Example of losing context
     Sets, lists, maps (set theoretic functions) are very basic and seem like
     they will never go away. If that's not a universal basis on which to build a
     future proof semantics, what is?

     100 years ago log tables were the primary means of computation. They were
     considered indispensible to the point that sci fi into the 60s still assumed
     space ships would have log tables that you would use to program the ship's
     computer (Spaceman Jones).

     Log tables have ceased to exist. Computers are so fast that we directly
     compute quantities from power series. Often without using logarithms at
     all. This would have been inconceivable in the past.

     Besides, ZFC is an ugly theory. You need choice to do many basic things, it
     leaves the continuum unsettled, it's just unsatisfying.  Assuming that it's
     too fundamental to be replaced is a failure of imagination.

     That said, future archaeologists will know that we used sets and maps
     and the rest and there will be books on the subject for historians of
     technology.

     It's the trail of context that needs to be maintained. Universality is a
     myth. Gödel proved that in 1931, but it still hasn't sunk in.
*** Perspective: Entropy (Cybernetics)
    A message is a thing given or received. Being a message is orthogonal to the
    idea of information.

    Remember that information is a probabilistic notion. The information in a
    message is the negative of its entropy, the unliklihood of its occurance.

    But probability is not an ontological notion. Probability is an
    epistemological proposition (Cf Jaynes 2003).

    So whether there is information to be had in a message or not, is a matter of
    context, a question of who receives the message.

    The entire field of cryptography can only exist because of this contextuality
    of information.

    What is the distribution from which messages are drawn? what does it mean for
    one message to be more likely than another? to have greater entropy? It means
    that given a prior, that is a given state of knowledge about the world, there
    are more configurations of the world leading to one message than another.

    That prior is exactly what I mean by context.

    Thermodynamics tells us that within a closed system, entropy always increases
    in the long run. That is to say that for any prior distribution (context),
    the posterior under observation of the system will approach the uniform in
    measure (this could use a lot more rigour) over the long haul.

    A Turing machine (going back to Turing (1936)) is a closed system. This point
    is often glossed over in CS classes and textbooks, but is incredibly
    important. See Wegner (1997) and Hewitt (2007).

    We take the perspective that a Turing machine is an information processing
    device — in the language of Weiner or Shannon which are close enough for our
    purposes to each other — and being a closed system, is a leaky information
    processing system.

    A Turing machine receives messages (input placed on the tape before running
    the machine) and emit messages (the state of the tape on completion). If the
    action of the Turing machine is invertible, that is the Turing machine
    defines an isomorphism from its input set to its output set, then the signal
    of output messages have the same entropy as the input. In all other cases,
    the entropy of the output must be strictly greater than that of the input. In
    other words, information is lost in interpretation by a Turing machine.

    This loss of information is independent of context. More precisely,
    information is lost no matter the context from which you define it. But the
    degree of loss may vary.

    But the Earth is not a closed system, and neither is anything on it except in
    certain, very artificial, situations.

    When you look up a word in a dictionary, you are reducing entropy. If that
    dictionary is on a website then the system of you, plus computer program,
    plus intervening network experiences an increase in information. But
    communication requires energy, which disipates as heat, so there is no
    violation of thermodynamics.

    This leads us to the inescapable conclusion that communication creates open
    systems and so a system of communicating components is something strictly
    more than a Turing machine.

    This observation isn't new, but it's widely dismissed as irrelevant. I hope
    to convince you otherwise.
*** Holons and Holarchy
    On the surface this system looks a lot like smalltalk, and that's not
    accidental.

    A program is a collection of programs (or computers) which communicate by
    sending messages to each other. That has a fractal beauty that is most fully
    realised (in my opinion) in the metaobject protocol of CLOS.

    Where I take issue with this approach is the freedom of communication. Any
    unit A can send a message to any other unit B, so long as the programmer who
    wrote A knew a name which resolves to B at runtime. Names take the place of
    symbols in linked object code; locations which introduce a disconnect
    between what the programmer thinks they're saying and what the machine
    thinks the programmer said.

    There's also a defiance of physical reality. Communication by knowing a name
    creates the illusion that all communication is equivalent, that any
    component can equally well communicate with any other component. But that
    isn't the case. Separate units running on CPUs and GPUs can't communicate
    with complete freedom. Barriers need to be put in place which slows down the
    computation, plus the cross talk latency is relatively high. The problem
    gets worse as we start to distribute programs over networks.

    Smalltalk was inspired by a biological metaphor, but in real life cells
    communicate by chemical signals which are 1) non-specific: everybody nearby
    hears every message (though not every cell exposed to a signal molecule
    reacts) and 2) local: chemical gradients get weaker by the inverse cube of
    the distance between cells. There are, of course, methods to extend this
    (hormones in the circulatory system, impulses in nerve fibres) but
    communication and coordination between distant cells is the exception,
    rather than the rule.

    So instead, I'm basing the design of this on what Koestler called holarchic
    organisation.

    Each flub (The word "object" used to be devoid of ideological baggage — and
    I suspect that's why it was used — but that's no longer the case) receives
    input on channels and emits output to channels. The flub has names for these
    channels since they must be referred to, but no knowledge of what's on the
    other side.

    This gives a flub autonomy from within. Given a set of inputs, the flub will
    do its thing, and that thing cannot be overridden or perverted from the
    outside. But since the flub has no notion of where inputs come from or where
    outputs go, when viewed from above it is fully subordinate to those "higher"
    flubs which decide how to network the channels of "lower" flubs together.

    Notably this removes the need for a global name registry, or "phone book" by
    which to route messages through the system. Flubs have references (by value)
    to other flubs, and connect them together, so names are only for the
    programmer's benefit. They resolve statically from the source itself (in
    context).

    Applying this idea recursively down the the language primitives themselves
    creates a nightmare not unlike dependency injection. I'm still looking for
    an elegant escape hatch.
** Note on Names
   Gregory Bateson gave the advice (I'm paraphrasing): when trying to think
   about things you don't understand and inventing concretions or abstractions
   that may or may not exist, may or may not be useful, to help yourself
   understand, give these concepts names and define them to the extent you can
   so that you can play with them; it's almost impossible to think deeply about
   something you can't name.

   But don't give them well thought out, expressive names. Don't give them names
   that might mislead others — or you yourself — into thinking that these are
   real and important. Give them stupid names so that you can discuss them
   without ever forgetting that they're provisional and likely bound for the
   dust bin of history.

   Note that Bateson said to use anglo saxon words for partially formed concepts
   and resist the urge to coin new greek or latin logisms. I like to use
   nonsense words or pet names. It keeps me honest.

   (Cf. his notion of ~ethos~, Experiments in Thinking about Observed
   Ethnological Material)
* A Language to Play with Theories
** Desiderata
   What do I want from an ideal language for playing with computing machinery?
*** Residential
    It's not a language I want, per se, it's an environment. A tightly coupled
    system of language-editor-tools that could potentially replace both emacs
    and the web browser.
*** Self sandboxing
    If units of execution are permitted no side effects, no mutations, no
    syscalls, no i/o except the arguments they recieve (as messages) and the
    messages they emit, then a runtime can look at (a subset of) a program and
    know what data can enter it, and where data leaving it might go.

    Thus a sandbox is just a program that runs another program (akin to a macro)
    which restricts access to certain information, or prevents leakage to
    untrusted locations.

    I'm mixing notions from service meshes into the program logic. But
    programming istio is an unmitigated clusterfuck, so if you can get those
    kinds of benefits using application code, and maintain the same boundaries
    between concerns, why shouldn't you?

    The ideal end state is to get a level of sandboxing that we can trust are as
    isolated as k8s pods but are (potentially) running in the same memory
    space.

    That will open up the door to using untrusted third party code safely, and
    more generally remixing applications freely: if I want to use code that
    assumes it's running in some specific context, I can mock that context and
    use the code in ways it was never intended to be.

    This is getting into potentially litigious territory, but I want to be able
    to override the default behaviour of apps on my phone without touching their
    source code. I want to be able to take a component from site A, another from
    site B, and a third of my own and combine them into a dashboard to suit my
    own purposes. Liberty is control over your own life.
*** Fully reflective
    Think Emacs, complete with the ability to override `self-insert-command` and
    brick everything.

    That brings up some interesting questions about undo (should you be allowed
    to break undo? can that be undone? how?) or at least restarting from a known
    state.
** Skepticism
   Which structures and patterns are useful, and which are just habit?
*** Dynamic Linking
    I got this idea originally from the [[https://www.unisonweb.org/][Unison]] language, but this is my
    interpretation and any faults herein are my own.

    A codebase is shared mutable state between developers. Uncoordinated
    changes by different developers, or by individuals at different points in
    time are the cause of a large class of bugs (git catches some of these as
    merge conflicts, but not all).

    I want to be able to modify code without fear of breaking anything I don't
    touch. If no existing code can change, then no existing functionality can
    break.

    In particular, this means that dynamic linking is unacceptable. The promise
    of dynamic linking is that bug fixes, security updates, and performance
    boosts will automagically trickle into your code as your dependencies
    release minor updates. The problem, of course, is that along with these
    come new bugs and breaking changes. We have a parallel with iatrogenics
    that puts us at the mercy of the gung-ho.

    Let's not forget that the real impetus that drove dynamic linking to become
    the standard was the fact that old machines didn't have enough drum or core
    space to hold much, so pieces had to be continually swapped in and
    out. That just isn't the case anymore.

    There's a synthesis of static and dynamic linking that I think gives us the
    best of both worlds. Given a reference by value scheme we can link code just
    as we do now, allowing shared libraries and small updates, but the links
    aren't symbols to be dumbly matched at runtime, they're unique references to
    specific bits of code that change if the code changes (think infinitely long
    hashes).

    But we still have to address the issue of updates. Security updates aren't
    going to go away anytime soon, so there needs to be a way to update large
    codebases wholesale.

    But given these references are explicit, a tool can scan and index
    them. Thus given a new version of some function, say SSL_do_handshake from
    openssl, the tooling can scan the entire codebase and say "These 7,453
    lines of code will be modified by this update, do you want to continue?".

    That sounds horrible, but is it worse than changing those 7000 loc and not
    even knowing it?
*** Types
    There are two ways in which the word "type" is used in programming.

    The first are the types in C, llvm, etc. which are just tags on chunks of
    bits. Given two machine words, how do you know that one is to be interpreted
    an an interger and the other as a sequence of unicode characters? You
    don't. To the hardware they're just words.

    The semantics (A is a long, B is a double) are separate from the syntax (the
    bitseqs themselves) by design. That was the entire goal of the Hilbertian
    formalist program, after all.

    But the idea that these types (semantics) need to be static is incidental
    baggage we're still carrying from the days when machine code had to be
    stored on and read off of punch cards, or a drum; there simply wasn't space
    to store words about words.

    It's not especially hard to write a program that looks at pairs of words and
    has a hard coded semantics — definitions must end somewhere — that uses the
    first word to know what to do with the second dynamically at runtime. It
    would double the size of the program in memory, but for many applications
    that's a non-issue.

    JIT compiling makes the proposition even simpler. As long as the runtime can
    figure out what words mean before passing code to the JIT, then the actual
    machine code being executed can be incredibly fast (this, and heavy caching,
    is the secret to Julia's impressive performance).

    The other use of "types" is to refer to type systems of the Hindley-Milner
    variety, and their descendents.

    These, frankly, don't interest me. Gödel showed that no such system can ever
    be expressive enough to encompass arithmetic, let alone the things I want to
    work with.

    The retort to Gödel currently in vogue is that any Gödelian proposition can
    be added to the system so that we can create a tower via iteration
    expressing whatever we want to express (this argument is ignorant of
    transfinite set theory, but let's leave that aside for now), thus solving
    incompleteness.

    Furthermore, consistency can be achieved via a tower of meta languages,
    where each one proves the one below it to be consistent (assuming it itself
    is consistent). This is an induction argument that can never have a base
    case, so it's fallacious, but in practice it actually works out pretty
    well. The meta languages get simpler and simpler until we're convinced they
    have to be consistent (or sometimes we can prove them consistent by other
    means).

    This is a lot of work. It's so much work that most people don't bother to do
    it properly. And if you're writing software whose greatest danger is
    someone's web browser crashing, it's simply not worth the effort.

    Don't get me wrong, if you're writing air traffic control software, or an
    autopilot for a car, you'd damn well better prove your software correct.

    But that's a small fraction of software, for most programs, proofs of
    correctness amount to Adams' 42.

    I'm also not convinced that logic is the most effective way to prove
    programs correct. In no other endeavour is logic used to construct proofs.
    Logic is a method of rigourously verifying proofs which already exist. And
    "rigour" is a target that has been moving through the entire history of
    mathematics.

    Software needs more Polya and less Plato, but we're a long ways off from
    that as yet.
*** The Stack
    The modern callstack and the prevalence of stack machines when defining
    languages has its origin in Dijkstra's Notes on Structured Programming
    (1970). (Cf. ALGOL 60, Interlisp-D, Forth,... Dijkstra didn't invent the use
    of stacks, but rather the modern paradigm of stack traces).

    Dijkstra's goal was to achieve a one to one correspondance between the text
    of the program and the instructions being executed in the hardware. He
    managed to do this with extreme elegance using just a stack and a couple of
    counters. It signaled certain death for the goto statement.

    And the stack works brilliantly for sequential, synchronous code. It works
    so well that stacks ops are part of the instruction set of modern chips, and
    students leave university thinking that stacks are an inherent part of
    programming languages.

    The problem, though, is that they suck at concurrency, especially in the
    face of asynchronicity.

    The problem is obvious if you ever worked in javascript pre ES6. It's
    also apparent in Rust's red/blue function kerfuffle when you realise the
    difference between red and blue is that one uses the stack and the other
    uses a scheduler / event loop.

    The program always needs to know where to go next, in particular functions
    need to know where to return to, but do we need to store this information on
    the stack?

    React is playing with the idea of virtualising the stack because when you
    have hundreds of ui tasks going on asyncronously and you want to interrupt,
    reorder, and resume them, when you need to modify or cancel them on the fly,
    then you need a different data structure.

    The early versions of Akka had a great hack to use the stack where is was
    beneficial and then blow it away: an actor would proceed like a normal
    function calling functions, until it hits a send call. Send would just build
    the current continuation, and throw an exception containing that
    continuation, the message, and the receiver. The scheduler catches that
    exception, queues the message and loops back. I always admired the
    cleverness of this approach.

    But concretely. I hypothesise that if we rethink the stack abstraction we
    can have asyncronous code that looks synchronous. Async/await without the
    keywords and dual nature.

    It should also help optimisers that want to reorder larger chunks of a
    program, or automatic parallelisation.

    Cf. Interlisp's "spaghetti stack" (actually a tree), which was manipulable
    as a first class data structure at runtime, allowing coroutining,
    continuations, backtracking, and other control flow operations to be
    implemented as library features. Try adding [[https://wiki.openjdk.org/display/loom/Main][coroutines to Java]]...

    Cf. core.async
*** Function, Proceedures, Coroutines, and Transducers
    Are function a good fundamental unit for programming?

    Can you guess what I think?

    Example: is (get m k) a pure function (clojure semantics, not a trick
    question)?

    The answer depends on whether you consider `nil` a first class thing.

    Hoare's null pointer blunder is due, at the end of the day, to the fact that
    (get m k) often has nothing meaningful to return. If k is not in m then
    there is no answer. But by the semantics of function calls, something must
    be returned. And so we reify nothing into null, nil, None,
    NotFoundException, etc..

    Type theory gives you a way of reifying nothing without the danger of null
    references, but it's still just a kludge to fix an older kludge.

    Why can't (get m k) just not return anything if it has nothing to return?

    Because functions always return a value. In set theory a function has a
    value for valid input, type theory lets you enforce this, but what is a
    reasonable k in (get m k)? Any value is a valid key, so should be part of
    the domain, but m is a finite map, so almost all inputs yield no valid
    output.

    So get is really a partial function whose domain depends on its first
    argument.

    Now what about `when`?

    Can we build a language that just short circuits instead of returning a
    reified nothing? Do nothing, don't say "nothing". `when` sends a message
    somewhere if its predicate comes back true, and if it comes back false, the
    current fibre of execution just dies and unwinds.

    If we add a mechanism to catch this unwinding, then we can build `if` from
    `when` and (get m k default) from (get m k). But by default it just unwinds
    all the way to the runtime and something else gets scheduled.

    So under the hood, these "functions" are proceedures that might jump to the
    return pointer when they finish, or might just `GOTO HALT`. Weird, but still
    structured in its own way.

    We now have "functions" that return zero or one value to the caller. Why
    stop there? A transducer is just such a "function" that passes on zero or
    more values for every input. It doesn't quite return to the caller, but
    we'll come back to that.

    Orthogonal issues: to whom to we "return" these values? and when?

    Conjecture: if we get the whom right, then when ceases to matter. This will
    take some justification in a separate point (see [[*The Myth of Synchoronicity][The Myth of Synchoronicity]]).

    A coroutine is a proceedure (aka routine) which decides for itself where
    control goes next. Instead of a call stack which decides what "return" means
    for you, (symmetric) coroutines end in a (yield X value) statement which
    says "send this value to X and give it control".

    I'm still trying to work out what a persistent (ie stateless) coroutine
    would look like at the assembly level. I'm pretty sure I want the solution
    to this problem, but it's not trivial and until I hit the point where I 100%
    need it, it only gets background thought privileges.

    Now take a toy program like
    =(fn [k] (let [v (get m k)] (when (even? v) (* v v))))=
    This says given a `k`, look it up and get back a (presumably) number, if
    it's even square it.

    What does (let [v (get m k)]) actually do? Is m local? does get park and
    wait for a remote server?

    It shouldn't matter. If we have control over where functions return, then
    `let` tells `get` send a value back to "here" (label?, call_cc()?), `get`
    then gets control and goes about its business. If it parks, `let` will be
    none the wiser, so long as `get` passes on the correct place to yield the
    eventual value.

    So if `get` finds a value to return that value finds its way back to the
    let statement which binds it to the name `v`, and control moves on to the
    body of `let`. Similarly if `when` decides to pass on control to its body
    then eventually `*` is passed `v` twice, does whatever it does and sends
    it's value to ???

    That's a good question. `*` should inherit its return pointer from `when`
    which inherits it from `let`, which in this case gets it off the return
    stack since we're invoking the `let` as the body of a function.

    Thus we get standard stack based funcall semantics even if get (and `even?`)
    actually have to park and wait for data. We have async handling without
    red/blue dichotomy or confusing keywords.

    But notice that we also get short circuiting. If `(get m k)` returns
    nothing, then we don't need to test `(even? nil)` because the computation
    just ends at `get`. We get a cheaper version of nil punning with no risk of
    using a null pointer because there is no null pointer. "nil" isn't a
    thing. We don't return "nothing", we don't return anything.

    But what if `m` is a multimap and `(get m k)` returns multiple values?

    N.B.: what follows is still actively in churn and I might consider it
    idiotic next week.

    One option is to require the programmer to have known that `m` was a
    multimap to begin with and plan for a collection at all points downstream.

    But blaming the user is too easy.

    A better option might be to fork the computation. Remember that coroutines
    are persistent and stateless, so each value `get` returns flows through the
    rest of computation (possibly in parallel) resulting in multiple return
    values getting passed back to the outermost caller. Note that this doesn't
    return a collection, if returns multiple values.

    If not everything returned from `get` is even, then the `when` statement
    acts like a `filter` transducer.

    This whole way of thinking about multiple return is inspired by transducers,
    but with immutability enforced at the lowest levels, these are all trivially
    parallelisable transductions.

    So multiple returns cause the computation to fork, nodes in the
    computational topology get replaced by lists of nodes (we should preserve
    order of messages even as things fork).

    This swelling of fibres of execution needs to be balanced by some form of
    joining. Aggregations like reduce are natural join points in the topology,
    but there won't always be a foldy step at the end, How to deal with this
    forking phenomenon in general needs more thought.

    Let's push a little further: is it reasonable to allow a "function" as
    defined above to only return to a single place? What if it has multiple
    messages (return values) that are fundamentally different and should go
    different places?

    As a practical example, how do you implement eval without mutability? Eval
    needs to keep a context of evaluation around (to store `def`ed things), but
    it also needs to return values to its caller. In a repl, eval must both send
    a message back to itself (recursively), and send a separate message to
    print. It can emit a pair, but then something downstream needs to split that
    pair and do two different things with it.

    Current idea: replace `yieldto` with `emit` but have a special form `fork`
    which takes zero or more emit statements. This is a low level construct that
    I don't see a way to avoid, but if it creeps its way into quotidian
    development, the language might be a failure. I'm not really sure yet.
*** The Myth of Synchoronicity
    What is the meaning of `async` in contemporary programming languages?
    It's a negation, and so only makes sense in relation to that which it's
    negating. Is there such a thing as `sync` to give it meaning? I happen to
    think it's a relic of legacy and habit.

    What's a fully synchronous operation? Adding two fixnums? Well are the
    numbers in registers or being loaded from ram? Are they in cache? Is the
    needed memory address paged out to disk? Has GC paused the world?

    And don't forget pipelining, speculation, and out of order execution.

    Synchronicity is an illusion we create so that we can visualise our programs
    as a linear sequence of instructions that happen one at a time and in
    order.

    All sync really means in this context is "while I'm waiting, nobody else can
    use the cpu". And in that sense, sync must die.

    So does this mean we can't meet hard real time requirements? Not at all. A
    program that assumes the cache will always hit is going to miss hard
    deadlines. A properly written real time program knows that even if the cache
    misses and everything possible goes wrong, the worst case bound is still
    acceptable.

    The goal is to bound the worst case, and the more we can do while waiting
    the better.

    Admittedly reasoning about async programs is harder because it's harder to
    pretend we know things we don't, and the scheduler brings in its own
    dynamics. But in the end, the more we admit the limits of our knowledge and
    work within them, the more reliable systems will be.
*** Start counting at 1
    The idea that real programmers start counting at zero comes from two
    related conflations. A conflation of cardinal and ordinal numbers, and a
    conflation of lists with allocated memory.

    When we learn to count in school we learn to start at 1. This is the first
    wug, this is the second wug, and so on... There is no zeroeth wug, but
    there can be zero wugs. That's the distinction between cardinality (the
    number of things in a set) and ordinality (the rank of something in a
    queue).

    Are array indicies cardinal numbers or ordinal numbers? That, like so much
    else in life, depends on the context. If you know that an array is a pair
    and you want to access the second element, then the index is ordinal. We
    want the second element, not the first element as there is no zeroeth
    element.

    But arrays aren't just lists. In modern computer architectures, memory is
    abstracted away as an enormous array. Everything you store has an address
    in this array, and we have to perform computations to find those addresses
    (which are really indicies).

    Say you have a pointer p to a struct {int32, int32, String} where we know
    that the second int is the length of the String (char array). The length of
    the string, say n,  is *(p+4) and the String itself is n bytes starting at
    *(p+8).

    Now we're doing arithmetic with array indicies. So in this case we're
    treating array indicies as cardinal numbers (you can define arithmetic on
    ordinals, but only set theorists ever do that).

    So why do real programmers start counting at zero? That's because if you're
    treating indicies as cardinal numbers, then you want the first thing (no
    offset) to be *(p+0).

    Take Dijkstra's famous argument regarding for (i=0; i<N; i++) {...}. This
    basically avoids having to fiddle with end conditions when concatenating
    arrays. Again this is about computing indicies and offsets.

    In the language being designed, there are only names and values. There are
    no explicit places. You can't say "find where x is stored and then give
    bytes ...", you can only refer to values that you know, or things whose
    names you know.

    Pointer arithmetic is out, so the need to facilitate it is gone.

    As for Dijkstra's example, modern languages don't use indicies to walk
    arrays anymore. All significant languages now provide a facility based on
    R.S. Bird's constructive programming theory. That is they use fold,
    reduce, iterators, whatever you want to call it. You should never be
    walking over a list with for (i=0; i<N; i++), so Dijkstra's argument is
    nowadays moot.

    Essentially, if you have a list of things, then they have an order, and
    that order is ordinal. You want the first, second, third, ... elements.

    If you need to compute an index nowadays then what you're really doing is
    constructing an indirect reference. The order of the things referenced is
    arbitrary and extrinsic. That means you aren't really talking about lists
    at all, you're talking about maps. Using arrays is an implementation concern
    based on current architectures.

    Confounding what we want to do with what we (incidentally) have to do
    creates inertia which prevents improvement of both our languages and our
    hardware. Linear RAM isn't the only way to build computers, but we have a
    feedback loop between low level programming languages which get performance
    by assuming things about the hardware which binds the hardware designers to
    meet the languages' expectations so that they stay fast which binds the
    language designers to make assumptions about hardware ... ad nauseum.

    Only put data in lists if it has an order which is important in some
    way. If order is arbitrary or otherwise unimportant, use a set or a map.
**** references
     Dijkstra (find the essay)
     Bird, constructive programming
* References
 - [[https://ia802307.us.archive.org/7/items/bitsavers_xeroxinternceManualOct1983_52302609/Interlisp_Reference_Manual_Oct_1983.pdf][Interlisp Reference Manual]]
 - [[https://dl.acm.org/doi/10.1145/592849.592858][Getting Erlang to Talk to the Outside World]]
 - The Art of the Metaobject Protocol
 - The Early History of Smalltalk
* Potentially Useful Reading
 - https://arxiv.org/abs/2206.01041
* Copyright
  © 2022 Thomas Getgood
